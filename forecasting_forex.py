# -*- coding: utf-8 -*-
"""forecasting_uns.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DjDYyKOlw1_XpfLrPQu4zFapNUZG4S1L
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Kaggle

!kaggle datasets download -d imetomi/eur-usd-forex-pair-historical-data-2002-2019

!ls

!unzip \*.zip  && rm *.zip

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler

data = pd.read_csv('eurusd_hour.csv')
data

# Merge date and hour coloumn into one coloumn called datetime
data['datetime'] = data['Date'] + " " + data['Hour']
data = data.drop(columns=['Date', 'Hour'])
data

# make datetime become index
data['datetime'] = pd.to_datetime(data['datetime'])
data.index = pd.to_datetime(data['datetime'])
data = data.drop(columns=['datetime'])
data

# convert hourly data to daily data with take the average/mean value in a day
data = data.resample('D').mean()
data

# checking whether there is null value or not and yeah there are so many missing value
data.isnull().sum()

# filling NaN value with mean of its row
for x in range(10):
  data.iloc[:,x].fillna(data.iloc[:,x].mean(), inplace = True)
  data.iloc[:,x]
data

data.isnull().sum()

data.describe()

dates = data.index.values
bid_close  = data['BidClose'].values
  
plt.figure(figsize=(25,10))
plt.plot(dates, bid_close)
plt.title('Bid close average',
          fontsize=25);

plt.figure(figsize=(15,5))
plt.plot(dates, bid_close, 'o-')
plt.title('Bid close average (3 hours interval)', fontsize=20)
plt.axis([dates[-150],dates[-1],0,50]);

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plt.hist(bid_close, bins=30)
plt.xlabel('Bid Close', fontsize=20)
plt.subplot(1,2,2)
aux = np.log( bid_close[1:] / bid_close[0:-1]  )
plt.hist(aux, bins=30)
plt.xlabel('Logarithmic bid close increment', fontsize=20)
plt.show()
print("Bid close average                      :", bid_close.mean())
print("Logarithmic bid close increment average:", aux.mean())

plt.figure(figsize=(15,5))
plt.plot(dates[1:], aux)
plt.title('Bid close (Logarithmic)',
          fontsize=20);

eps = 1e-11

NAN = np.NAN

# Logarithmic transformation

def transform_logratios(serie):
    aux = np.log((serie[1:]+eps) / (serie[0:-1]+eps))
    return np.hstack( ([NAN], aux))
def inverse_transform_logratios(log_ratio, temp_prev):
    return np.multiply(temp_prev, np.exp(log_ratio))

transform = transform_logratios
inverse_transform = inverse_transform_logratios

# normalization
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data.loc[:, ["BidOpen","BidHigh","BidLow", "BidChange", "AskOpen", "AskHigh", "AskLow", "AskClose", "AskChange"]])

scaled_data = pd.DataFrame(scaled_data, columns = ["BidOpen","BidHigh","BidLow", "BidChange", "AskOpen", "AskHigh", "AskLow", "AskClose", "AskChange"])
scaled_data.index = data.index
scaled_data['BidClose'] = data['BidClose']
scaled_data

# splitting the data to train and test set

train_size = int(len(scaled_data) * 0.8)
test_size = len(scaled_data) - train_size
train, test = scaled_data.iloc[0:train_size], scaled_data.iloc[train_size:len(scaled_data)]
print(len(train), len(test))

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 10

# reshape to [samples, time_steps, n_features]
X_train, y_train = create_dataset(train, train['BidClose'], time_steps)
X_test, y_test = create_dataset(test, test['BidClose'], time_steps)
print(X_train.shape, y_train.shape)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

# for vizualisation loss and accuracy
def show_final_history(history):
    fig, ax = plt.subplots(1, 2, figsize=(15,5))
    ax[0].set_title('loss')
    ax[0].plot(history.epoch, history.history["loss"], label="Train loss")
    ax[0].plot(history.epoch, history.history["val_loss"], label="Validation loss")
    ax[1].set_title('mae')
    ax[1].plot(history.epoch, history.history["mae"], label="Train mae")
    ax[1].plot(history.epoch, history.history["val_mae"], label="Validation mae")
    ax[0].legend()
    ax[1].legend()

from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler
# Callbacks
best_model_weights = './base.model'
checkpoint = ModelCheckpoint(
    best_model_weights,
    monitor='val_loss',
    verbose=1,
    save_best_only=True,
    mode='min',
    save_weights_only=False,
)
earlystop = EarlyStopping(
    monitor='val_loss',
    min_delta=0.001,
    patience=10,
    verbose=1,
    mode='auto'
)
tensorboard = TensorBoard(
    log_dir = './logs',
    histogram_freq=0,
    write_graph=True,
    write_images=False,
)

csvlogger = CSVLogger(
    filename= "training_csv.log",
    separator = ",",
    append = False
)

reduce = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=40,
    verbose=1, 
    mode='auto',
    cooldown=1 
)

callbacks = [checkpoint,tensorboard,csvlogger,reduce]

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
# history = model.fit(train_set,epochs=15)


history = model.fit(
    X_train, y_train,
    epochs=15,
    batch_size=16,
    validation_split=0.1,
    verbose=1,
    shuffle=False,
    callbacks=callbacks
)

show_final_history(history)
model_json = model.to_json()

y_pred = model.predict(X_test)